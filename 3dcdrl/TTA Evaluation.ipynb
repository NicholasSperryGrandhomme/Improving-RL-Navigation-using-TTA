{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TTA Evaluation.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1NBBq5wlg5rZ70iRd7CP2xLu0Wwlyrt6l","authorship_tag":"ABX9TyPUlYjxnVuwhLNGHFPiaiD+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51PedKaH5-Di","executionInfo":{"status":"ok","timestamp":1639937836728,"user_tz":-60,"elapsed":2121,"user":{"displayName":"Nicholas Sperry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17478191746282896441"}},"outputId":"e5c3a498-be9c-46c7-fbd9-ac7bbad96a5c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/My Drive/EPFL/Visual Intelligence/A2C'\n","%cd 3dcdrl/"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/EPFL/Visual Intelligence/A2C\n","/content/drive/My Drive/EPFL/Visual Intelligence/A2C/3dcdrl\n"]}]},{"cell_type":"code","metadata":{"id":"cJ71x_Im7Rcj"},"source":["!sudo apt install cmake libboost-all-dev libsdl2-dev libfreetype6-dev libgl1-mesa-dev libglu1-mesa-dev libpng-dev libjpeg-dev libbz2-dev libfluidsynth-dev libgme-dev libopenal-dev zlib1g-dev timidity tar nasm\n","!pip install vizdoom"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The majority of the code used in this project can be found below:\n","* [Agent and Project Structure](https://github.com/edbeeching/3d_control_deep_rl)\n","* [Test Time Adaptation](https://github.com/nicklashansen/policy-adaptation-during-deployment)"],"metadata":{"id":"uiHfXNM-kaZD"}},{"cell_type":"markdown","source":["# Keep training PRE-Trained Model"],"metadata":{"id":"-OM02cKDNJ_k"}},{"cell_type":"markdown","source":["Keep training the Pre-Trained Navigation Agent while using TTA. Add the `--use_tta` flag followed by the type of tta, either `--use_rot` or `use_gray`. The flag `--num_environments` essentially determines the batch size. `--hidden_size 128` must be used."],"metadata":{"id":"dvnJ0j4_jyEO"}},{"cell_type":"code","source":["!python train_agent.py --num_environments 16 --reload_model saved_models/labyrinth_9\\_checkpoint_0198658048.pth.tar,0 --hidden_size 128 --skip_eval --num_frames 1000000 --scenario custom_scenario{:003}.cfg --num_stack 1 --limit_actions --scenario_dir scenarios/custom_scenarios/labyrinth/9/train/ --test_scenario_dir scenarios/custom_scenarios/labyrinth/9/test/ --multimaze --num_mazes_train 16 --num_mazes_test 4 --fixed_scenario"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVtSvjbwRE1M","executionInfo":{"status":"ok","timestamp":1639328789305,"user_tz":-60,"elapsed":2030221,"user":{"displayName":"Nicholas Sperry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17478191746282896441"}},"outputId":"28abf513-a666-4010-f146-884fdb70fddb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(alpha=0.99, conv1_size=16, conv2_size=32, conv3_size=16, disable_head_bob=False, entropy_coef=0.001, eps=1e-05, eval_freq=1000, eval_games=50, fixed_scenario=True, frame_skip=4, gamma=0.99, hidden_size=128, job_id=12345, learning_rate=0.0007, limit_actions=True, log_interval=100, max_grad_norm=0.5, max_iters=5000000, model_checkpoint='', model_dir='', model_save_rate=1000, momentum=0.0, multimaze=True, num_actions=0, num_environments=16, num_frames=1000000, num_mazes_test=4, num_mazes_train=16, num_stack=1, num_steps=128, out_dir='./', reload_model='saved_models/labyrinth_9_checkpoint_0198658048.pth.tar,0', resize=True, scenario='custom_scenario{:003}.cfg', scenario_dir='scenarios/custom_scenarios/labyrinth/9/train/', screen_height=64, screen_size='320X180', screen_width=112, show_window=False, simulator='doom', skip_eval=True, stoc_evals=False, tau=0.95, test_name='test_000', test_scenario='', test_scenario_dir='scenarios/custom_scenarios/labyrinth/9/test/', train_freq=4, train_report_freq=100, use_em_loss=False, use_gae=False, use_pipes=False, use_shaping=False, use_visdom=False, value_loss_coef=0.5, visdom_ip='http://10.0.0.1', visdom_port=8097)\n","Created log output directory ./tmp/results/doom_rl/12345_test_000_2021_12_12_16_32/\n","========== Training Log file ==========\n","Device:  cpu\n","Total number of iterations:  488\n","scenarios, chunk size\n","16 1\n","Starting process:  0\n","Starting process:  1\n","Starting process:  2\n","Starting process:  3\n","Starting process:  4\n","Starting process:  5\n","Starting process:  6\n","Starting process:  7\n","Starting process:  8\n","Starting process:  9\n","Starting process:  10\n","Starting process:  11\n","Starting process:  12\n","Starting process:  13\n","Starting process:  14\n","Starting process:  15\n","There are 16 workers\n","scenarios, chunk size\n","4 0\n","There are 0 workers\n","Observation Shape:  (3, 64, 112)\n","torch.Size([1, 3, 64, 112]) torch.Size([1, 16, 4, 10]) torch.Size([1, 16, 4, 10])\n","odict_keys(['conv_head.0.weight', 'conv_head.0.bias', 'conv_head.2.weight', 'conv_head.2.bias', 'conv_head.4.weight', 'conv_head.4.bias', 'linear1.weight', 'linear1.bias', 'gru.weight_ih', 'gru.weight_hh', 'gru.bias_ih', 'gru.bias_hh', 'critic_linear.weight', 'critic_linear.bias', 'dist.linear.weight', 'dist.linear.bias'])\n","Total number of iterations:  488\n","Starting iteration:  1\n","Number of steps per iteration:  128\n","Starting training...\n","Iteration:  1\n","Iteration:  2\n","Iteration:  3\n","Iteration:  4\n","Iteration:  5\n","Iteration:  6\n","Iteration:  7\n","Iteration:  8\n","Iteration:  9\n","Iteration:  10\n","Iteration:  11\n","Iteration:  12\n","Iteration:  13\n","Iteration:  14\n","Iteration:  15\n","Iteration:  16\n","Iteration:  17\n","Iteration:  18\n","Iteration:  19\n","Iteration:  20\n","Iteration:  21\n","Iteration:  22\n","Iteration:  23\n","Iteration:  24\n","Iteration:  25\n","Iteration:  26\n","Iteration:  27\n","Iteration:  28\n","Iteration:  29\n","Iteration:  30\n","Iteration:  31\n","Iteration:  32\n","Iteration:  33\n","Iteration:  34\n","Iteration:  35\n","Iteration:  36\n","Iteration:  37\n","Iteration:  38\n","Iteration:  39\n","Iteration:  40\n","Iteration:  41\n","Iteration:  42\n","Iteration:  43\n","Iteration:  44\n","Iteration:  45\n","Iteration:  46\n","Iteration:  47\n","Iteration:  48\n","Iteration:  49\n","Iteration:  50\n","Iteration:  51\n","Iteration:  52\n","Iteration:  53\n","Iteration:  54\n","Iteration:  55\n","Iteration:  56\n","Iteration:  57\n","Iteration:  58\n","Iteration:  59\n","Iteration:  60\n","Iteration:  61\n","Iteration:  62\n","Iteration:  63\n","Iteration:  64\n","Iteration:  65\n","Iteration:  66\n","Iteration:  67\n","Iteration:  68\n","Iteration:  69\n","Iteration:  70\n","Iteration:  71\n","Iteration:  72\n","Iteration:  73\n","Iteration:  74\n","Iteration:  75\n","Iteration:  76\n","Iteration:  77\n","Iteration:  78\n","Iteration:  79\n","Iteration:  80\n","Iteration:  81\n","Iteration:  82\n","Iteration:  83\n","Iteration:  84\n","Iteration:  85\n","Iteration:  86\n","Iteration:  87\n","Iteration:  88\n","Iteration:  89\n","Iteration:  90\n","Iteration:  91\n","Iteration:  92\n","Iteration:  93\n","Iteration:  94\n","Iteration:  95\n","Iteration:  96\n","Iteration:  97\n","Iteration:  98\n","Iteration:  99\n","Iteration:  100\n","Updates 100, num timesteps 206848, FPS (495,), mean/median reward 0.9/1.0, min/max reward -0.2/1.0, entropy 0.65978, value loss 0.05240, policy loss -0.00618, TTA Rotation Loss 0.007\n","Saving model...\n","Iteration:  101\n","Iteration:  102\n","Iteration:  103\n","Iteration:  104\n","Iteration:  105\n","Iteration:  106\n","Iteration:  107\n","Iteration:  108\n","Iteration:  109\n","Iteration:  110\n","Iteration:  111\n","Iteration:  112\n","Iteration:  113\n","Iteration:  114\n","Iteration:  115\n","Iteration:  116\n","Iteration:  117\n","Iteration:  118\n","Iteration:  119\n","Iteration:  120\n","Iteration:  121\n","Iteration:  122\n","Iteration:  123\n","Iteration:  124\n","Iteration:  125\n","Iteration:  126\n","Iteration:  127\n","Iteration:  128\n","Iteration:  129\n","Iteration:  130\n","Iteration:  131\n","Iteration:  132\n","Iteration:  133\n","Iteration:  134\n","Iteration:  135\n","Iteration:  136\n","Iteration:  137\n","Iteration:  138\n","Iteration:  139\n","Iteration:  140\n","Iteration:  141\n","Iteration:  142\n","Iteration:  143\n","Iteration:  144\n","Iteration:  145\n","Iteration:  146\n","Iteration:  147\n","Iteration:  148\n","Iteration:  149\n","Iteration:  150\n","Iteration:  151\n","Iteration:  152\n","Iteration:  153\n","Iteration:  154\n","Iteration:  155\n","Iteration:  156\n","Iteration:  157\n","Iteration:  158\n","Iteration:  159\n","Iteration:  160\n","Iteration:  161\n","Iteration:  162\n","Iteration:  163\n","Iteration:  164\n","Iteration:  165\n","Iteration:  166\n","Iteration:  167\n","Iteration:  168\n","Iteration:  169\n","Iteration:  170\n","Iteration:  171\n","Iteration:  172\n","Iteration:  173\n","Iteration:  174\n","Iteration:  175\n","Iteration:  176\n","Iteration:  177\n","Iteration:  178\n","Iteration:  179\n","Iteration:  180\n","Iteration:  181\n","Iteration:  182\n","Iteration:  183\n","Iteration:  184\n","Iteration:  185\n","Iteration:  186\n","Iteration:  187\n","Iteration:  188\n","Iteration:  189\n","Iteration:  190\n","Iteration:  191\n","Iteration:  192\n","Iteration:  193\n","Iteration:  194\n","Iteration:  195\n","Iteration:  196\n","Iteration:  197\n","Iteration:  198\n","Iteration:  199\n","Iteration:  200\n","Updates 200, num timesteps 411648, FPS (493,), mean/median reward 1.0/1.0, min/max reward 0.9/1.0, entropy 0.62373, value loss 0.05971, policy loss 0.00442, TTA Rotation Loss 0.180\n","Saving model...\n","Iteration:  201\n","Iteration:  202\n","Iteration:  203\n","Iteration:  204\n","Iteration:  205\n","Iteration:  206\n","Iteration:  207\n","Iteration:  208\n","Iteration:  209\n","Iteration:  210\n","Iteration:  211\n","Iteration:  212\n","Iteration:  213\n","Iteration:  214\n","Iteration:  215\n","Iteration:  216\n","Iteration:  217\n","Iteration:  218\n","Iteration:  219\n","Iteration:  220\n","Iteration:  221\n","Iteration:  222\n","Iteration:  223\n","Iteration:  224\n","Iteration:  225\n","Iteration:  226\n","Iteration:  227\n","Iteration:  228\n","Iteration:  229\n","Iteration:  230\n","Iteration:  231\n","Iteration:  232\n","Iteration:  233\n","Iteration:  234\n","Iteration:  235\n","Iteration:  236\n","Iteration:  237\n","Iteration:  238\n","Iteration:  239\n","Iteration:  240\n","Iteration:  241\n","Iteration:  242\n","Iteration:  243\n","Iteration:  244\n","Iteration:  245\n","Iteration:  246\n","Iteration:  247\n","Iteration:  248\n","Iteration:  249\n","Iteration:  250\n","Iteration:  251\n","Iteration:  252\n","Iteration:  253\n","Iteration:  254\n","Iteration:  255\n","Iteration:  256\n","Iteration:  257\n","Iteration:  258\n","Iteration:  259\n","Iteration:  260\n","Iteration:  261\n","Iteration:  262\n","Iteration:  263\n","Iteration:  264\n","Iteration:  265\n","Iteration:  266\n","Iteration:  267\n","Iteration:  268\n","Iteration:  269\n","Iteration:  270\n","Iteration:  271\n","Iteration:  272\n","Iteration:  273\n","Iteration:  274\n","Iteration:  275\n","Iteration:  276\n","Iteration:  277\n","Iteration:  278\n","Iteration:  279\n","Iteration:  280\n","Iteration:  281\n","Iteration:  282\n","Iteration:  283\n","Iteration:  284\n","Iteration:  285\n","Iteration:  286\n","Iteration:  287\n","Iteration:  288\n","Iteration:  289\n","Iteration:  290\n","Iteration:  291\n","Iteration:  292\n","Iteration:  293\n","Iteration:  294\n","Iteration:  295\n","Iteration:  296\n","Iteration:  297\n","Iteration:  298\n","Iteration:  299\n","Iteration:  300\n","Updates 300, num timesteps 616448, FPS (493,), mean/median reward 0.9/1.0, min/max reward -0.2/1.0, entropy 0.60826, value loss 0.04777, policy loss -0.01549, TTA Rotation Loss 0.014\n","Saving model...\n","Iteration:  301\n","Iteration:  302\n","Iteration:  303\n","Iteration:  304\n","Iteration:  305\n","Iteration:  306\n","Iteration:  307\n","Iteration:  308\n","Iteration:  309\n","Iteration:  310\n","Iteration:  311\n","Iteration:  312\n","Iteration:  313\n","Iteration:  314\n","Iteration:  315\n","Iteration:  316\n","Iteration:  317\n","Iteration:  318\n","Iteration:  319\n","Iteration:  320\n","Iteration:  321\n","Iteration:  322\n","Iteration:  323\n","Iteration:  324\n","Iteration:  325\n","Iteration:  326\n","Iteration:  327\n","Iteration:  328\n","Iteration:  329\n","Iteration:  330\n","Iteration:  331\n","Iteration:  332\n","Iteration:  333\n","Iteration:  334\n","Iteration:  335\n","Iteration:  336\n","Iteration:  337\n","Iteration:  338\n","Iteration:  339\n","Iteration:  340\n","Iteration:  341\n","Iteration:  342\n","Iteration:  343\n","Iteration:  344\n","Iteration:  345\n","Iteration:  346\n","Iteration:  347\n","Iteration:  348\n","Iteration:  349\n","Iteration:  350\n","Iteration:  351\n","Iteration:  352\n","Iteration:  353\n","Iteration:  354\n","Iteration:  355\n","Iteration:  356\n","Iteration:  357\n","Iteration:  358\n","Iteration:  359\n","Iteration:  360\n","Iteration:  361\n","Iteration:  362\n","Iteration:  363\n","Iteration:  364\n","Iteration:  365\n","Iteration:  366\n","Iteration:  367\n","Iteration:  368\n","Iteration:  369\n","Iteration:  370\n","Iteration:  371\n","Iteration:  372\n","Iteration:  373\n","Iteration:  374\n","Iteration:  375\n","Iteration:  376\n","Iteration:  377\n","Iteration:  378\n","Iteration:  379\n","Iteration:  380\n","Iteration:  381\n","Iteration:  382\n","Iteration:  383\n","Iteration:  384\n","Iteration:  385\n","Iteration:  386\n","Iteration:  387\n","Iteration:  388\n","Iteration:  389\n","Iteration:  390\n","Iteration:  391\n","Iteration:  392\n","Iteration:  393\n","Iteration:  394\n","Iteration:  395\n","Iteration:  396\n","Iteration:  397\n","Iteration:  398\n","Iteration:  399\n","Iteration:  400\n","Updates 400, num timesteps 821248, FPS (493,), mean/median reward 1.0/1.0, min/max reward 0.9/1.0, entropy 0.62381, value loss 0.04842, policy loss -0.02920, TTA Rotation Loss 0.010\n","Saving model...\n","Iteration:  401\n","Iteration:  402\n","Iteration:  403\n","Iteration:  404\n","Iteration:  405\n","Iteration:  406\n","Iteration:  407\n","Iteration:  408\n","Iteration:  409\n","Iteration:  410\n","Iteration:  411\n","Iteration:  412\n","Iteration:  413\n","Iteration:  414\n","Iteration:  415\n","Iteration:  416\n","Iteration:  417\n","Iteration:  418\n","Iteration:  419\n","Iteration:  420\n","Iteration:  421\n","Iteration:  422\n","Iteration:  423\n","Iteration:  424\n","Iteration:  425\n","Iteration:  426\n","Iteration:  427\n","Iteration:  428\n","Iteration:  429\n","Iteration:  430\n","Iteration:  431\n","Iteration:  432\n","Iteration:  433\n","Iteration:  434\n","Iteration:  435\n","Iteration:  436\n","Iteration:  437\n","Iteration:  438\n","Iteration:  439\n","Iteration:  440\n","Iteration:  441\n","Iteration:  442\n","Iteration:  443\n","Iteration:  444\n","Iteration:  445\n","Iteration:  446\n","Iteration:  447\n","Iteration:  448\n","Iteration:  449\n","Iteration:  450\n","Iteration:  451\n","Iteration:  452\n","Iteration:  453\n","Iteration:  454\n","Iteration:  455\n","Iteration:  456\n","Iteration:  457\n","Iteration:  458\n","Iteration:  459\n","Iteration:  460\n","Iteration:  461\n","Iteration:  462\n","Iteration:  463\n","Iteration:  464\n","Iteration:  465\n","Iteration:  466\n","Iteration:  467\n","Iteration:  468\n","Iteration:  469\n","Iteration:  470\n","Iteration:  471\n","Iteration:  472\n","Iteration:  473\n","Iteration:  474\n","Iteration:  475\n","Iteration:  476\n","Iteration:  477\n","Iteration:  478\n","Iteration:  479\n","Iteration:  480\n","Iteration:  481\n","Iteration:  482\n","Iteration:  483\n","Iteration:  484\n","Iteration:  485\n","Iteration:  486\n","Iteration:  487\n","workers cancelled\n","workers cancelled\n"]}]},{"cell_type":"markdown","source":["## Pre-Trained Evaluation"],"metadata":{"id":"vlltRQB2NOs4"}},{"cell_type":"markdown","source":["### Flags\n","There are various maps you can choose from, which are listed below:\n","    \n","* ***original.cfg*** - Original map\n","* ***mossy_walls.cfg*** - Walls have a mossy texture\n","* ***decreased_light.cfg*** - Map is darker\n","* ***increased_light.cfg*** - Map is brighter\n","* ***swapped_ceil_floor.cfg*** - Floor and Ceiling textures swapped, and map is brighter.\n"," \n","To choose one, use the `--scenario` flag followed by the file name, e.g. `--scenario original.cfg`.\n","\n","You can increase the gamma of the images by using `--gamma` flag followed by the amount, e.g. `--gamma 1.5`. The higher the gamma the darker the image\n","\n","To inverse the image, use the `--inverse` flag.\n","\n","To name the output video and the saved times per run, use the `--exp_name` flag followed by whatever you want to name the file\n","\n","### Test Time Adaptation\n","To enable TTA during evaluation, use the `--use_tta` flag followed by the type of TTA, either `--use_rot` for TTA with rotation prediction or `use_gray` for TTA with grayscale prediction. \n","\n","### Available maps\n","The available maps were mentioned above. Those are the ones we tested the baseline and TTA methods on. If you want more, you can download the maps from [here](https://github.com/edbeeching/3d_control_deep_rl/tree/master/3dcdrl/scenarios/custom_scenarios/labyrinth/9/test)."],"metadata":{"id":"SnaxczqMroJt"}},{"cell_type":"code","source":["!!python create_rollout_videos.py --limit_actions \\\n","       --scenario_dir  scenarios/custom_scenarios/labyrinth/9/test/ \\\n","       --scenario swapped_ceil_floor.cfg  --model_checkpoint \\\n","       tta_models/policy.pth.tar \\\n","       --multimaze --num_mazes_test 1 --num_environments 1 --num_actions 5 \\\n","       --use_tta --use_gray\n"],"metadata":{"id":"GCvsz0vCMeaZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639942860141,"user_tz":-60,"elapsed":193724,"user":{"displayName":"Nicholas Sperry","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17478191746282896441"}},"outputId":"385f4918-4c43-4d23-fde0-6b724a18a322"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"Namespace(alpha=0.99, conv1_size=16, conv2_size=32, conv3_size=16, disable_head_bob=False, entropy_coef=0.001, eps=1e-05, eval_freq=1000, eval_games=50, experiment_name='original', fixed_scenario=False, frame_skip=4, gamma=0.99, gamma_val=1, hidden_size=128, inverse=False, job_id=12345, learning_rate=0.0007, limit_actions=True, log_interval=100, max_grad_norm=0.5, max_iters=5000000, model_checkpoint='tta_models/policy.pth.tar', model_dir='', model_save_rate=1000, momentum=0.0, multimaze=True, num_actions=5, num_environments=1, num_frames=200000000, num_mazes_test=1, num_mazes_train=16, num_stack=1, num_steps=128, out_dir='./', reload_model='', resize=True, scenario='swapped_ceil_floor.cfg', scenario_dir='scenarios/custom_scenarios/labyrinth/9/test/', screen_height=64, screen_size='320X180', screen_width=112, show_window=False, simulator='doom', skip_eval=False, stoc_evals=False, tau=0.95, test_name='test_000', test_scenario='', test_scenario_dir='', train_freq=4, train_report_freq=100, use_em_loss=False, use_gae=False, use_gray=False, use_pipes=False, use_rot=False, use_shaping=False, use_tta=False, use_visdom=False, value_loss_coef=0.5, visdom_ip='http://10.0.0.1', visdom_port=8097)\",\n"," 'scenarios/custom_scenarios/labyrinth/9/test/swapped_ceil_floor.cfg',\n"," 'baseline',\n"," 'Creating movie TTA_videos/baseline/original.mp4',\n"," '525',\n"," 'Average time taken: 75.00s',\n"," 'TTA mean loss: nan',\n"," '[MoviePy] >>>> Building video TTA_videos/baseline/original.mp4',\n"," '[MoviePy] Writing video TTA_videos/baseline/original.mp4',\n"," '',\n"," '  0% 0/526 [00:00<?, ?it/s]',\n"," ' 17% 87/526 [00:00<00:00, 858.32it/s]',\n"," ' 33% 173/526 [00:00<00:00, 605.78it/s]',\n"," ' 45% 238/526 [00:00<00:00, 536.85it/s]',\n"," ' 56% 295/526 [00:00<00:00, 525.68it/s]',\n"," ' 66% 349/526 [00:00<00:00, 529.37it/s]',\n"," ' 77% 403/526 [00:00<00:00, 523.80it/s]',\n"," ' 87% 456/526 [00:00<00:00, 500.48it/s]',\n"," ' 97% 508/526 [00:00<00:00, 502.78it/s]',\n"," '100% 526/526 [00:00<00:00, 532.35it/s]',\n"," '[MoviePy] Done.',\n"," '[MoviePy] >>>> Video ready: TTA_videos/baseline/original.mp4 ',\n"," '']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":[""],"metadata":{"id":"Y0ETUw6k3AeA"},"execution_count":null,"outputs":[]}]}